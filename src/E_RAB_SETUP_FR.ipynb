{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xuqKjkLqt5_X"
   },
   "source": [
    "# Investigation of the Bayesian and non-Bayesian Time Series Forcasting Fraemworks in Application to OSS Systems of the LTE/LTE-A and 5G Mobile Networks \n",
    "\n",
    "This Jupyter Notebook belongs to the following paper:\n",
    "\n",
    "> Fadeev V.A., Zaidullin S.V., Nadeev A.F. (2022). [Investigation of the Bayesian and non-Bayesian time series forecasting frameworks in application to OSS systems of the LTE/LTE-A and 5G mobile networks](http://media-publisher.ru/wp-content/uploads/Nom-4-2022s.pdf). T-Comm, vol. 16, no.4, pÑ€. 52-60.\n",
    "\n",
    "**Autors of the code**:\n",
    "- Vladimir Fadeev ([kirlf](https://github.com/kirlf))\n",
    "- Shakhrozy Zaidullin ([shortferd](https://github.com/shortferd))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "D-jOSdt5VN1R",
    "outputId": "9ff30a56-bca0-4248-8771-2953dd002c3c"
   },
   "outputs": [],
   "source": [
    "!pip install hmmlearn==0.2.5\n",
    "!pip install pmdarima==1.8.5\n",
    "!pip install pydlm==0.1.1.11\n",
    "!pip install pomegranate==0.14.8\n",
    "!pip install xgboost==1.6.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WZ2NHxId_3pi"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yposlX_aAldp"
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from pandas.plotting import register_matplotlib_converters\n",
    "register_matplotlib_converters()\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "from pylab import rcParams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "78p7ECkgVN1V"
   },
   "outputs": [],
   "source": [
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "from pmdarima.arima import ARIMA\n",
    "from sklearn.svm import SVR\n",
    "from xgboost import XGBRegressor, plot_importance\n",
    "from pydlm import dlm, dynamic, seasonality, autoReg, modelTuner\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6mmN4y3sVN1X"
   },
   "outputs": [],
   "source": [
    "from pomegranate import MultivariateGaussianDistribution as mvgauss\n",
    "from pomegranate import HiddenMarkovModel as hmm_pom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_uiK564dImQC"
   },
   "outputs": [],
   "source": [
    "# Features selections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-xNvPhwtIqvA"
   },
   "outputs": [],
   "source": [
    "def create_features(df, label=None):\n",
    "    \n",
    "    \"\"\"\n",
    "    Creates time series features from datetime index.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df: pandas.DataFrame\n",
    "        Input data frame.\n",
    "    label: str (default None)\n",
    "        Name of the considered KPI.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pandas.DataFrame\n",
    "        Output data frame.\n",
    "    \"\"\"\n",
    "    \n",
    "    new_df = df.copy()\n",
    "    new_df.index = df[\"DT\"]\n",
    "    new_df['date'] = new_df.index\n",
    "    new_df['hour'] = new_df['date'].dt.hour\n",
    "    new_df['day_of_year'] = new_df['date'].dt.dayofyear\n",
    "\n",
    "    X = new_df[['hour', 'day_of_year']]\n",
    "\n",
    "    #new_df['day_of_week'] = new_df['date'].dt.dayofweek\n",
    "    #new_df['quarter'] = new_df['date'].dt.quarter\n",
    "    #new_df['month'] = new_df['date'].dt.month\n",
    "    #new_df['day_of_month'] = new_df['date'].dt.day\n",
    "    #new_df['week_of_year'] = new_df['date'].dt.isocalendar().week\n",
    "    \n",
    "    #X = new_df[['hour','day_of_week','quarter','month',\n",
    "    #       'day_of_year','day_of_month','week_of_year']]\n",
    "        \n",
    "    if label:\n",
    "        y = df[label]\n",
    "        return X, y\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hqZZ3Z_lJHIC"
   },
   "outputs": [],
   "source": [
    "\n",
    "class ModelTrainer:\n",
    "    \"\"\"\n",
    "    Class for models training automatization.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    model_type: str\n",
    "    One of the available models name or abrivation:\n",
    "      - 'HW' (Holt-Winters model)\n",
    "      - 'SARIMA'\n",
    "      - 'SARIMAX'\n",
    "      - 'SARIMA (Kalman)'\n",
    "      - 'SARIMAX (Kalman)'\n",
    "      - 'DLM' (Dynamical Linear Models)\n",
    "      - 'SVR'\n",
    "      - 'XGboost'\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                model_type, \n",
    "                scale_target=False):\n",
    "      \n",
    "      self.model_type = model_type\n",
    "\n",
    "    @staticmethod\n",
    "    def __holt_winters_fp(y_train, X_test, **model_kwargs):\n",
    "        \"\"\" Holt-Winter's method (triple exponential smoothing) \"\"\"\n",
    "\n",
    "        old_index = X_test.index\n",
    "        X_test.reset_index(inplace=True)\n",
    "\n",
    "        seasonal_periods = model_kwargs.get(\"seasonal_periods\", 24)\n",
    "        model = ExponentialSmoothing(y_train,\n",
    "                                     trend=\"add\",\n",
    "                                     seasonal=\"add\", \n",
    "                                     seasonal_periods=seasonal_periods).fit()\n",
    "\n",
    "        predictions = model.predict(start=X_test.index[0], end=X_test.index[-1])\n",
    "        X_test.set_index(old_index, inplace=True)\n",
    "        return predictions\n",
    "\n",
    "    @staticmethod\n",
    "    def __sarimax_fp(y_train, X_test, X_train=None, **model_kwargs):\n",
    "        \"\"\" SARIMAX model \n",
    "        (Seasonal Autoregressive Integrated Moving Averaging \n",
    "        with exogenous variables)\n",
    "        \"\"\"\n",
    "\n",
    "        seasonal_periods = model_kwargs.get(\"seasonal_periods\", 24)\n",
    "        mle_regression = model_kwargs.get(\"mle_regression\", True)\n",
    "        sarimax_kwargs = {\"mle_regression\": mle_regression}\n",
    "\n",
    "        # Based on the pre-estimation using AutoARIMA (takes several hours)\n",
    "        order = (2, 0, 1) \n",
    "        seasonal_order = (1, 0, 2, seasonal_periods)\n",
    "\n",
    "        model = ARIMA(order, \n",
    "                      seasonal_order=seasonal_order,\n",
    "                      **sarimax_kwargs).fit(y_train, \n",
    "                                            X=X_train)\n",
    "\n",
    "        if X_train is not None:\n",
    "            predictions = model.predict(X=X_test, n_periods=X_test.shape[0])\n",
    "        else:\n",
    "             predictions = model.predict(n_periods=X_test.shape[0])\n",
    "\n",
    "        return predictions\n",
    "\n",
    "    @staticmethod\n",
    "    def __pydlm_fp(y_train, X_test, X_train, **model_kwargs):\n",
    "        \"\"\" Dynamical linear models\n",
    "        \"\"\"\n",
    "\n",
    "        seasonal_periods = model_kwargs.get(\"seasonal_periods\", 24)\n",
    "\n",
    "        dynamic_comp = dynamic(features=[list(i) for i in X_train.to_numpy()], \n",
    "                                             name=\"time_series_feature\")\n",
    "\n",
    "        seasonality_comp = seasonality(period=seasonal_periods)\n",
    "\n",
    "        model = dlm(y_train) + dynamic_comp + seasonality_comp + autoReg()\n",
    "        model.fit()\n",
    "\n",
    "        feature_dict = {\"time_series_feature\": [list(i) for i in X_test.to_numpy()]}\n",
    "        predictions, vars = model.predictN(N=X_test.shape[0], \n",
    "                                           featureDict=feature_dict)\n",
    "\n",
    "        return predictions\n",
    "\n",
    "    @staticmethod\n",
    "    def __svr_fp(y_train, X_test, X_train, **model_kwargs):\n",
    "\n",
    "        \"\"\" Support vector regression \"\"\"\n",
    "\n",
    "        # This option is used extarnal scaler\n",
    "\n",
    "        y_scaler = StandardScaler()\n",
    "        X_scaler = StandardScaler()\n",
    "\n",
    "        X_train = X_scaler.fit_transform(X_train)\n",
    "        X_test = X_scaler.fit_transform(X_test)\n",
    "        y_train = y_scaler.fit_transform(y_train.values.reshape(-1,1))\n",
    "\n",
    "        model = SVR(kernel=\"rbf\", \n",
    "                    C=100, \n",
    "                    gamma=0.1, \n",
    "                    epsilon=0.1).fit(X_train, y_train)\n",
    "\n",
    "        predictions_scaled = model.predict(X_test)\n",
    "        predictions = y_scaler.inverse_transform(predictions_scaled.reshape(-1,1))\n",
    "\n",
    "        return predictions\n",
    "\n",
    "    def fit_predict(self, \n",
    "                  y_train,\n",
    "                  X_test, \n",
    "                  X_train=None,\n",
    "                  **model_kwargs):\n",
    "\n",
    "        \"\"\" \n",
    "        Fits the model and predicts the values \n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "\n",
    "        y_train: 1-D array\n",
    "          Training target vector.\n",
    "\n",
    "        X_test: 2-D array\n",
    "          Testing features matrix.\n",
    "\n",
    "        X_test: 2-D array\n",
    "          Training features matrix.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "\n",
    "        predictions: 1-D array\n",
    "          Predicted values.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "        if self.model_type == \"HW\":\n",
    "            predictions = self.__holt_winters_fp(y_train, \n",
    "                                               X_test, \n",
    "                                               **model_kwargs)\n",
    "\n",
    "        elif self.model_type == \"SARIMAX\":\n",
    "            predictions = self.__sarimax_fp(y_train, \n",
    "                                          X_test, \n",
    "                                          X_train, \n",
    "                                          **model_kwargs)\n",
    "        elif self.model_type == \"SARIMA\":\n",
    "            predictions = self.__sarimax_fp(y_train, \n",
    "                                          X_test, \n",
    "                                          **model_kwargs)\n",
    "\n",
    "        elif self.model_type == \"SARIMAX (Kalman)\":\n",
    "            model_kwargs.update({\"mle_regression\": False})\n",
    "            predictions = self.__sarimax_fp(y_train, \n",
    "                                          X_test, \n",
    "                                          **model_kwargs)\n",
    "\n",
    "        elif self.model_type == \"SARIMA (Kalman)\":\n",
    "            model_kwargs.update({\"mle_regression\": False})\n",
    "            predictions = self.__sarimax_fp(y_train, \n",
    "                                          X_test,\n",
    "                                          X_train,\n",
    "                                          **model_kwargs)\n",
    "\n",
    "        elif self.model_type == \"DLM\":\n",
    "            predictions = self.__pydlm_fp(y_train, \n",
    "                                        X_test, \n",
    "                                        X_train, \n",
    "                                        **model_kwargs)\n",
    "\n",
    "        elif self.model_type == \"SVR\":\n",
    "            predictions = self.__svr_fp(y_train, \n",
    "                                      X_test, \n",
    "                                      X_train)\n",
    "\n",
    "        elif self.model_type == \"XGBoost\":\n",
    "            model = XGBRegressor(n_estimators=1000).fit(X_train, y_train)\n",
    "            predictions = model.predict(X_test)\n",
    "\n",
    "        return predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3i6Lo-1hAdqZ"
   },
   "outputs": [],
   "source": [
    "## Specify the name of KPI\n",
    "KPI = \"E_RAB_SETUP_FR\"\n",
    "\n",
    "## Specify year\n",
    "year_v = 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "id": "3ym5yC1uEiY8",
    "outputId": "9bb3d708-007f-475e-a3e8-b29d3189c4d1"
   },
   "outputs": [],
   "source": [
    "## Extraction ot the data\n",
    "path = \"./data/\"\n",
    "df = pd.read_excel(f\"{path}{KPI}_LTE.xlsx\", sheet_name=str(year_v))\n",
    "df = df[[\"DT\", KPI]]\n",
    "df = df.loc[(df[\"DT\"] >= \"01.01.%d\" % year_v) & (df[\"DT\"] < \"01.01.%d\" % (year_v+1))]\n",
    "df = (df.set_index('DT').reindex(pd.date_range(start=f'1/1/{year_v}', end=f'31/12/{year_v}', freq='H'))).fillna(method='ffill')\n",
    "df[\"DT\"] = df.index\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 535
    },
    "id": "qrkZDwdsFaiO",
    "outputId": "aab0c5d8-278c-4ee1-fe24-d5f32b74a889"
   },
   "outputs": [],
   "source": [
    "## Visualization of the time series single-sided\n",
    "plt.figure(figsize=(14, 8))\n",
    "plt.plot(df[\"DT\"], df[KPI])\n",
    "plt.grid(True)\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"%s (%s)\" % (KPI, \"%\"))\n",
    "plt.title(\"%s (single-sided)\" % KPI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 535
    },
    "id": "gi5MOjJsFtL0",
    "outputId": "ad7329f5-a68d-488d-8437-c31c241623b6"
   },
   "outputs": [],
   "source": [
    "daily = df.copy()\n",
    "daily[\"DT\"] = pd.to_datetime(daily[\"DT\"])\n",
    "daily = daily.loc[(df[\"DT\"] >= \"21.07.%d\" % year_v) & (df[\"DT\"] <= \"24.07.%d\" % year_v)]\n",
    "\n",
    "plt.figure(figsize=(14, 8))\n",
    "plt.plot(daily[\"DT\"], daily[KPI])\n",
    "plt.grid(True)\n",
    "plt.xlabel(\"Date and time\")\n",
    "plt.ylabel(\"%s (%s)\" % (KPI, \"%\"))\n",
    "plt.title(\"%s (several days)\" % KPI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 540
    },
    "id": "tNh_fkL9F2An",
    "outputId": "d7accdd1-e1c8-4229-b774-c5bef4f6aa23"
   },
   "outputs": [],
   "source": [
    "df['year'] = [d.year for d in df.DT]\n",
    "df['month'] = [d.strftime('%b') for d in df.DT]\n",
    "years = df['year'].unique()\n",
    "\n",
    "# Draw Plot\n",
    "fig, axes = plt.subplots(1, 1, figsize=(10,7), dpi= 80)\n",
    "sns.boxplot(x='month', y=KPI, data=df)\n",
    "\n",
    "# Set Title\n",
    "axes.set_title('Month-wise Box Plot\\n(The Seasonality)', fontsize=18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3hJI0i83GvV4"
   },
   "source": [
    "# Regular part forecasting\n",
    "\n",
    "Models to research:\n",
    "- Continuous State-space (PyDLM)\n",
    "- Holts-Winter's method (MLE and Kalman based training)\n",
    "- SARIMAX (MLE and Kalman based training)\n",
    "- Polynomial regression\n",
    "- Support Vector Regression\n",
    "- XGBoost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fwn8ZFiMG1BS"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import (mean_absolute_error, \n",
    "                             median_absolute_error, \n",
    "                             mean_squared_error)\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "\n",
    "def timeseries_cv_score(series, \n",
    "                        KPI,\n",
    "                        model_name,\n",
    "                        n_splits,\n",
    "                        sc_X = None,\n",
    "                        sc_y = None,\n",
    "                        on_index=True,\n",
    "                        **model_kwargs):\n",
    "\n",
    "    \"\"\" Time series cross validation. \n",
    "        This function prints out the following scores:\n",
    "          - Median Absolute Error\n",
    "          - Mean Absolute Error (MAE)\n",
    "          - Mean Squared Error (MSE)\n",
    "          - Root Mean Squared Error (RMSE)\n",
    "    \n",
    "    Parameters\n",
    "    __________\n",
    "    \n",
    "    series: pandas.DataFrame\n",
    "      Input data frame.\n",
    "    \n",
    "    KPI: str\n",
    "      Name of the column to use (considered Key Performance Identifier).\n",
    "    \n",
    "    model_name: object\n",
    "      inititalizes the instance of the explored model class\n",
    "    \n",
    "    n_splits: int\n",
    "      How many splits should be done for the cross-validation procedure.\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    print(f\"\\n{model_name}\")\n",
    "    \n",
    "    # errors array\n",
    "    median_aes = []\n",
    "    maes = []\n",
    "    mses = []\n",
    "    rmses = []    \n",
    "    \n",
    "    # set the number of folds for cross-validation\n",
    "    tscv = TimeSeriesSplit(n_splits=n_splits) \n",
    "    \n",
    "    # iterating over folds, train model on each, forecast and calculate error\n",
    "    for train_indexes, test_indexes in tscv.split(series):\n",
    "        \n",
    "        X_train, y_train = create_features(series.iloc[train_indexes], \n",
    "                                           label=KPI) # training sets\n",
    "        X_test, y_test = create_features(series.iloc[test_indexes], \n",
    "                                         label=KPI) # testing sets\n",
    "        \n",
    "        \n",
    "        # Fit the model and predict the values \n",
    "        predictions = ModelTrainer(model_name).fit_predict(y_train, \n",
    "                                                           X_test, \n",
    "                                                           X_train,\n",
    "                                                           **model_kwargs)\n",
    "\n",
    "        # Errors counting\n",
    "        \n",
    "        median_aes.append(median_absolute_error(predictions, y_test))\n",
    "        maes.append(mean_absolute_error(predictions, y_test))\n",
    "        mses.append(mean_squared_error(predictions, y_test))\n",
    "        rmses.append(mean_squared_error(predictions, y_test, squared=False))\n",
    "    \n",
    "    print(\"Median AE: \", np.mean(np.array(median_aes)))\n",
    "    print(\"MAE: \", np.mean(np.array(maes)))\n",
    "    print(\"MSE: \", np.mean(np.array(mses)))\n",
    "    print(\"RMSE: \", np.mean(np.array(rmses)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hmeJXwcLgZFE",
    "outputId": "d4e7a481-c73e-4950-c634-9a481510dd6e"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ARIMA takes couple of hours\n",
    "\n",
    "#cases = [\"SARIMAX\", \"SARIMAX (Kalman)\", \"SARIMA\", \"SARIMA (Kalman)\", \n",
    "#         \"SVR\", \"HW\", \"XGBoost\", \"DLM\"]\n",
    "\n",
    "cases = [\"SVR\", \"HW\", \"XGBoost\", \"DLM\"]\n",
    "\n",
    "for model_name in cases:\n",
    "    timeseries_cv_score(df, KPI, model_name, n_splits=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4iTHC8rftIGK"
   },
   "outputs": [],
   "source": [
    "# import warnings\n",
    "# warnings.filterwarnings('ignore')\n",
    "\n",
    "# # ARIMA takes couple of hours\n",
    "\n",
    "# cases = [\"SARIMAX\", \"SARIMAX (Kalman)\", \"SARIMA\", \"SARIMA (Kalman)\"]\n",
    "\n",
    "# for model_name in cases:\n",
    "#   timeseries_cv_score(df, KPI, model_name, n_splits=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wgUqpFcNvP6o"
   },
   "source": [
    "# Outliers prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lHvB6-EBygrO"
   },
   "outputs": [],
   "source": [
    "## Specify the name of KPI\n",
    "KPI = \"E_RAB_SETUP_FR\"\n",
    "path = \"./data/\"\n",
    "\n",
    "dfs = {}\n",
    "for sheet in (\"2017\", \"2018\", \"2019\"):\n",
    "    dfs.update({sheet: pd.read_excel(f\"{path}{KPI}_LTE.xlsx\", \n",
    "                                     sheet_name=sheet)})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D3SVuO5Bybtz"
   },
   "outputs": [],
   "source": [
    "def calc_inner_join(real_part, prediction):\n",
    "\n",
    "    \"\"\"\n",
    "    Calculates relations between observation and prediction.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    real_part: 1-D array\n",
    "    Observed vector.\n",
    "    prediction: 1-D array\n",
    "    Predicted vector.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "    Keys\n",
    "    ----\n",
    "      intersections: int\n",
    "        Number of correctly predicted values.\n",
    "      false_alarms: int\n",
    "        Number of false alarms (outliers).\n",
    "      missed_alarms: int\n",
    "        Number of missed alarms (outliers).\n",
    "      detected_alarms: int\n",
    "        Number of detected in observation outliers.\n",
    "      predicted_alarms: int\n",
    "        Number of detected in prediction outliers.\n",
    "    \"\"\"\n",
    "\n",
    "    inner_join_count = np.intersect1d(real_part, prediction).shape[0]\n",
    "\n",
    "    real_part = set(real_part)\n",
    "    prediction = set(prediction)\n",
    "\n",
    "    len_real = len(list(real_part))\n",
    "    len_model = len(list(prediction))\n",
    "\n",
    "    print(f\"Lenght real: {len_real}\")\n",
    "    print(f\"Length prediction: {len_model}\")\n",
    "\n",
    "\n",
    "    false_alarm = sum([1 for pr in prediction if pr not in real_part])\n",
    "    missed_alarm = sum([1 for pr in real_part if pr not in prediction]) \n",
    "\n",
    "    print(f\"Number of intersections: {inner_join_count}\")\n",
    "    print(f\"Number of false alarms: {false_alarm}\")\n",
    "    print(f\"Number of missed alarms: {missed_alarm}\")\n",
    "\n",
    "    return {\"intersections\": inner_join_count,\n",
    "          \"false_alarms\": false_alarm,\n",
    "          \"missed_alarms\": missed_alarm, \n",
    "          \"detected_alarms\": len_real,\n",
    "          \"predicted_alarms\": len_model}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KuY6YmQBzPrg"
   },
   "outputs": [],
   "source": [
    "def find_problem_state(states):\n",
    "    \"\"\" Suppose that the number of outliers is smaller \n",
    "    than number of 'regular' samples \"\"\"\n",
    "    unique, counts = np.unique(states, return_counts=True)\n",
    "    counts_d = {val: count for val, count in zip(unique, counts)}\n",
    "    return min(counts_d, key=counts_d.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xvVUgi8ozFta"
   },
   "outputs": [],
   "source": [
    "def outliers_investigation(train_df, \n",
    "                           test_df, \n",
    "                           test_df_states, \n",
    "                           KPI, \n",
    "                           model_name,\n",
    "                           **model_kwargs):\n",
    "    \"\"\"\n",
    "    Runs outliers prediction investigations.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "\n",
    "    train_df: 2-D array\n",
    "    Training data set.\n",
    "\n",
    "    test_df: 2-D array\n",
    "    Testing data set.\n",
    "\n",
    "    test_df_states: 1-D array.\n",
    "    States that are detected in observations.\n",
    "\n",
    "    KPI: str\n",
    "    Considered KPI.\n",
    "\n",
    "    model_name: str\n",
    "    Considered model.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "    Keys\n",
    "    ----\n",
    "    predictions: 2-D array\n",
    "      Data frame that contains predictions.\n",
    "    res_samples: dict\n",
    "      Results of prediction checking for sample number as a criterion.\n",
    "    res_days: dict\n",
    "      Results of prediction checking for day of year as a criterion.\n",
    "    \"\"\"\n",
    "\n",
    "    print(f\"\\n{model_name}\")\n",
    "\n",
    "    # Observed states\n",
    "    df_predicted = pd.DataFrame()\n",
    "    df_predicted[KPI] = test_df[KPI]\n",
    "    df_predicted[\"Rows number\"] = [i for i in range(df_predicted.shape[0])]\n",
    "    df_predicted[\"DT\"] = test_df[\"DT\"]\n",
    "    df_predicted[\"test states\"] = test_df_states\n",
    "\n",
    "    observed_problem_state = find_problem_state(df_predicted[\"test states\"])\n",
    "    alarm_observed = df_predicted.loc[df_predicted[\"test states\"] == observed_problem_state]\n",
    "\n",
    "\n",
    "    # Features\n",
    "    X_train, y_train = create_features(train_df, \n",
    "                                     label=KPI) # training sets\n",
    "    X_test, y_test = create_features(test_df, \n",
    "                                   label=KPI) # testing sets\n",
    "\n",
    "\n",
    "    # Fit the model and predict the values\n",
    "\n",
    "    predictions = ModelTrainer(model_name).fit_predict(y_train, \n",
    "                                                     X_test, \n",
    "                                                     X_train,\n",
    "                                                     **model_kwargs)\n",
    "    # Plot\n",
    "    test_df_copy = test_df.copy()\n",
    "    if model_name == \"HW\":\n",
    "        predictions = list(predictions)\n",
    "    test_df_copy[\"predictions\"] = predictions\n",
    "\n",
    "\n",
    "    # plt.plot(test_df_copy[KPI])\n",
    "    # plt.plot(test_df_copy[\"predictions\"])\n",
    "    # plt.show()\n",
    "\n",
    "    # Predicted states\n",
    "    hmm_model_from_samp = hmm_pom.from_samples(distribution = mvgauss, \n",
    "                                             n_components=2, \n",
    "                                             X=[predictions],\n",
    "                                             algorithm='baum-welch')\n",
    "\n",
    "\n",
    "    states_prediction = hmm_model_from_samp.predict(sequence=test_df[KPI])\n",
    "    df_predicted[\"predicted states\"] = states_prediction\n",
    "    predicted_problem_state = find_problem_state(df_predicted[\"predicted states\"])\n",
    "    alarm_predicted = df_predicted.loc[df_predicted[\"predicted states\"] == predicted_problem_state]\n",
    "\n",
    "\n",
    "    # Samples\n",
    "    print(\"Samples\")\n",
    "    res_samp = calc_inner_join(alarm_observed[\"Rows number\"], \n",
    "                             alarm_predicted[\"Rows number\"])\n",
    "\n",
    "    # Days\n",
    "    print(\"Days of year\")\n",
    "    res_days = calc_inner_join(alarm_observed[\"DT\"].dt.dayofyear, \n",
    "                             alarm_predicted[\"DT\"].dt.dayofyear)\n",
    "\n",
    "    df_predicted[\"predictions\"] = predictions\n",
    "    return {\"predictions\": df_predicted,\n",
    "          \"res_samples\": res_samp,\n",
    "          \"res_days\": res_days}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ltybBs8xJc1T"
   },
   "outputs": [],
   "source": [
    "def prepare_df(df, year):\n",
    "    df = (df.set_index('DT').reindex(pd.date_range(start=f'1/1/{year}', \n",
    "                                                 end=f'1/1/{int(year)+1}', \n",
    "                                                 freq='H'))).fillna(method='ffill')\n",
    "    df[\"DT\"] = df.index\n",
    "    df = df.loc[(df[\"DT\"] >= f\"01.01.{year}\") & (df[\"DT\"] < f\"01.01.{int(year) + 1}\" )]\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UhiMtd85OyXR"
   },
   "outputs": [],
   "source": [
    "def print_results_otliers(result):\n",
    "  \n",
    "    print(f\"Number of intersections: {np.mean(result['intersections'])}\")\n",
    "    print(f\"Number of false alarms: {np.mean(result['false_alarms'])}\")\n",
    "    print(f\"Number of missed alarms: {np.mean(result['missed_alarms'])}\")\n",
    "    print(f\"Number of detected alarms: {np.mean(result['detected_alarms'])}\")\n",
    "    print(f\"Number of prdicted alarms: {np.mean(result['predicted_alarms'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1IrciieM72cz",
    "outputId": "4f14c505-c9ab-4b9c-e7cf-473b866d33ad"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ARIMA takes couple of hours\n",
    "\n",
    "#cases = [\"SARIMAX\", \"SARIMAX (Kalman)\", \"SARIMA\", \"SARIMA (Kalman)\", \n",
    "#         \"SVR\", \"HW\", \"XGBoost\", \"DLM\"]\n",
    "\n",
    "cases = [\"SVR\", \"HW\", \"XGBoost\", \"DLM\"]\n",
    "\n",
    "cv_cases = [{\"train\": (\"2017\", \"2018\"),\n",
    "             \"test\": \"2019\"},\n",
    "            {\"train\": (\"2017\", \"2019\"),\n",
    "             \"test\": \"2018\"},\n",
    "            {\"train\": (\"2018\", \"2019\"),\n",
    "             \"test\": \"2017\"}]\n",
    "\n",
    "models_result = {i: {\"samples\": {\"intersections\": [],\n",
    "                                 \"false_alarms\": [],\n",
    "                                 \"missed_alarms\": [],\n",
    "                                 \"detected_alarms\": [], \n",
    "                                 \"predicted_alarms\": []},\n",
    "                     \"days\": {\"intersections\": [],\n",
    "                              \"false_alarms\": [],\n",
    "                              \"missed_alarms\": [],\n",
    "                              \"detected_alarms\": [], \n",
    "                              \"predicted_alarms\": []}\n",
    "                } for i in cases}\n",
    "\n",
    "\n",
    "for case in cv_cases:\n",
    "  \n",
    "    train_years = [prepare_df(dfs[year], year) for year in case[\"train\"]]\n",
    "    df_train = pd.concat(train_years)\n",
    "\n",
    "    df_test = prepare_df(dfs[case[\"test\"]], case[\"test\"])\n",
    "\n",
    "    hmm_model_from_samp = hmm_pom.from_samples(distribution = mvgauss, \n",
    "                                             n_components=2, \n",
    "                                             X=[df_test[KPI]],\n",
    "                                             algorithm='baum-welch')\n",
    "    states_observed = hmm_model_from_samp.predict(sequence=df_test[KPI])\n",
    "\n",
    "    for model_name in cases:\n",
    "\n",
    "        result = outliers_investigation(df_train, \n",
    "                            df_test,\n",
    "                            states_observed,\n",
    "                            KPI,\n",
    "                            model_name)\n",
    "\n",
    "        for i, j in zip ((\"samples\", \"days\"), (\"res_samples\", \"res_days\")):\n",
    "            for field in (\"intersections\", \n",
    "                        \"false_alarms\", \n",
    "                        \"missed_alarms\", \n",
    "                        \"detected_alarms\", \n",
    "                        \"predicted_alarms\"):\n",
    "                models_result[model_name][i][field].append(result[j][field])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1xjxFnawPaVB",
    "outputId": "192be41d-4c60-41fc-8a4d-2f5175f435c5"
   },
   "outputs": [],
   "source": [
    "for model in [\"SVR\", \"HW\", \"XGBoost\", \"DLM\"]:\n",
    "    print(model)\n",
    "    for case, title in zip((\"samples\", \"days\"), (\"Samples\", \"Day of year\")):\n",
    "        print(title)\n",
    "        print_results_otliers(models_result[model][case])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gUi3ZM0g8bvv"
   },
   "outputs": [],
   "source": [
    "# # This models cannot preddict outliers\n",
    "# cases = [\"SARIMAX\", \"SARIMAX (Kalman)\"]\n",
    "\n",
    "# for model_name in cases:\n",
    "#   outliers_investigation(df_train, \n",
    "#                          df,\n",
    "#                          states_observed,\n",
    "#                          KPI,\n",
    "#                          model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vagxP2IE_5QN"
   },
   "outputs": [],
   "source": [
    "# # This models cannot preddict outliers\n",
    "# cases = [\"SARIMA\", \"SARIMA (Kalman)\"]\n",
    "# for model_name in cases:\n",
    "#   outliers_investigation(df_train, \n",
    "#                          df,\n",
    "#                          states_observed,\n",
    "#                          KPI,\n",
    "#                          model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zoOBk8hBWTyu"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "E_RAB_SETUP_FR.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
